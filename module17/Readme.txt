Summary of results from module 17 - Practical Application 3:

In this project, I compared several classification models—Logistic Regression, KNN, Decision Trees, and SVM—using a real-world marketing dataset from a bank. The goal was to understand how well these models can predict whether a client will subscribe to a term deposit after being contacted by phone. One of the main themes throughout the analysis is how model evaluation changes when the data is imbalanced, and why accuracy on its own can be misleading in this type of problem.

The business problem is centered on helping the bank better target its marketing efforts. Only a small fraction of clients actually subscribe to a term deposit, so simply predicting “no” most of the time can look good from an accuracy standpoint while still being completely unhelpful in practice. A useful model needs to correctly identify at least some of the clients who are likely to subscribe, even if that comes at the cost of making more mistakes overall.

The dataset comes from the UCI ML Repository and includes roughly 41k client records spanning 17 marketing campaigns. The target variable indicates whether a client subscribed to a term deposit. For this analysis, I focused only on basic bank client attributes such as age, job, marital status, education level, and loan or default status. Campaign-specific and macroeconomic variables were intentionally excluded early on to keep the initial models simple and easier to understand and tune.

The modeling process followed a standard machine learning workflow. I started by establishing a baseline using a dummy classifier that always predicts the most frequent outcome, which in this case is that the client does not subscribe. This provided a reference point for evaluating whether more complex models were actually adding value. I then trained several base-case models using default hyperparameters, including Logistic Regression, KNN, Decision Trees, and SVM, and evaluated them on a held-out test set.

Initially, models were evaluated using accuracy, which revealed that many of them achieved around 88–89% accuracy. However, closer inspection showed that several of these models were effectively just predicting the majority class and failing to identify any subscribers at all. To better capture performance on the minority class, I introduced F1 score as an alternative evaluation metric. This made the limitations of the base models much more apparent and highlighted meaningful differences in model behavior that accuracy alone had hidden.

Based on the base-case results, I focused further improvements on Logistic Regression and Decision Trees. 
--Logistic Regression provides a stable and interpretable baseline, but required class-imbalance handling to become useful. By tuning hyperparameters and introducing class weighting, Logistic Regression showed a significant improvement in classification performance, with F1 increasing from approximately 0.00 to around 0.26. This improvement came with a drop in accuracy, which is expected once the model begins predicting positive cases more often. 
--Decision Trees, on the other hand, were able to identify minority-class signal even in the base case, but consistently overfit the training data. Hyperparameter tuning reduced neither overfitting nor improved test-set F1 in a meaningful way.

Overall, this analysis shows that accuracy alone is not sufficient when outcomes are rare, and that handling class imbalance matters more than increasing model complexity. Among the models tested, a tuned Logistic Regression model provided the most practical balance between performance and interpretability. Future improvements would likely come from incorporating richer features, such as campaign-level or behavioral data, adjusting classification thresholds based on business costs, or exploring ensemble methods designed specifically for imbalanced datasets.
